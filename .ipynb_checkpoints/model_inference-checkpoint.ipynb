{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import sys\n",
    "sys.path.append('./helpers/')\n",
    "from scipy.io import loadmat\n",
    "from data_manip import *\n",
    "from temporal_bases import *\n",
    "from compiled_models import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle5 as pkl\n",
    "import os\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      " stimulus shape: (31, 1, 21600, 50)\n",
      " spikes shape: (6, 21600, 50)\n"
     ]
    }
   ],
   "source": [
    "# Training data must be in the following format:\n",
    "# stimulus: (nx, ny, nt, n_rep) for n_rep movies of 2D stimuli of length nt, eventually n_rep can be 1\n",
    "# spikes: (n_cells, nt, n_rep) for n_cells neurons\n",
    "\n",
    "# Load training data\n",
    "with open('./data/' + 'data_bars_OFF_unrepeated' + '.pkl', 'rb') as f:\n",
    "    data_train = pkl.load(f)\n",
    "    \n",
    "spikes = data_train['spikes'][:,:,:]\n",
    "stimulus = data_train['stimulus'][:,:,:,:]\n",
    "print(f'Training data:\\n',\n",
    "    f'stimulus shape: {stimulus.shape}\\n',\n",
    "      f'spikes shape: {spikes.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 1 ... 0 0 0]\n",
      "   [0 0 1 ... 0 0 0]\n",
      "   [0 0 1 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 1 ... 0 0 0]\n",
      "   [0 0 1 ... 0 0 0]\n",
      "   [0 0 1 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 1 ... 0 0 0]\n",
      "   [0 0 1 ... 0 0 0]\n",
      "   [0 0 1 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]]]]\n",
      "Flattened stimulus shape: (31, 21600, 50)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the spatial dimensions of the stimulus\n",
    "print(stimulus)\n",
    "n_cells = spikes.shape[0]\n",
    "nx, ny, nt, n_rep = stimulus.shape\n",
    "stimulus = np.reshape(stimulus, [nx*ny, nt, n_rep], order='F')\n",
    "\n",
    "print(f'Flattened stimulus shape: {stimulus.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the dataset: 865.325927734375\n",
      "Split in 1 parts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 16:58:27.170217: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-07 16:58:27.170254: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-07 16:58:27.170286: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-06-07 16:58:27.170897: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "### GLM inference\n",
    "\n",
    "# Pretrained model\n",
    "load_pretrained = False\n",
    "path_weights = './model_weights/GLM/'\n",
    "\n",
    "# Split the dataset in parts of size < to:\n",
    "max_size_dataslice = 1024 # in Mo\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "nepochs = 50\n",
    "n_epochs_stim_freeze = 40 # Freeze the stimulus filter and continue couplings optimization\n",
    "\n",
    "# Storage\n",
    "models = []\n",
    "losses = np.zeros((n_cells, nepochs))\n",
    "\n",
    "# Parameters of the temporal bases\n",
    "first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim = 1, 170, 100, 6, 200\n",
    "first_peak_coupl, last_peak_coupl, streach_coupl, n_basis_coupl, nt_integ_coupl = 0, 15, 5, 4, 25\n",
    "last_peak_self, streach_self, n_basis_self, nt_integ_self = 20, 1, 5, 25\n",
    "\n",
    "# Stimulus basis\n",
    "stim_basis = raised_cosine_basis(first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim)\n",
    "# Coupling basis\n",
    "coupl_basis = raised_cosine_basis(first_peak_coupl, last_peak_coupl, streach_coupl, n_basis_coupl, nt_integ_coupl)\n",
    "\n",
    "for cell in range(n_cells):\n",
    "    # Self coupling basis\n",
    "    self_basis, tau_r = self_basis_gen(last_peak_self, streach_self, n_basis_self, nt_integ_self, cell, spikes)\n",
    "    \n",
    "    # Build the training dataset\n",
    "    n_parts_dataset = build_dataset(stimulus, spikes, max_size_dataslice, stim_basis, 'GLM',\n",
    "                                 cell, coupl_basis, self_basis, tau_r)\n",
    "    # If the whole dataset fits in memory, load it once\n",
    "    if n_parts_dataset == 1:\n",
    "        dataset_full = load_dataset(model='GLM', cell=cell, batch_size=batch_size, ipart=0, n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self)\n",
    "\n",
    "    # Create the model\n",
    "    n_basis_stim, nt_integ_stim = stim_basis.shape\n",
    "    n_basis_coupl = coupl_basis.shape[0]\n",
    "    n_basis_self = self_basis.shape[0]\n",
    "\n",
    "    temp_model = compiled_GLM_model(nx=nx, ny=ny, n_cells=n_cells, n_basis_stim=n_basis_stim,\n",
    "                                   n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self,\n",
    "                                   l1_reg_stim=0, l2_reg_stim=0, l2_lapl_reg=2e-3, lapl_axis='all', \n",
    "                                   l1_reg_coupl=1e-5, l2_reg_coupl=0, l1_reg_self=1e-5, l2_reg_self=0)\n",
    "\n",
    "    # Load pretrained model weights\n",
    "    if load_pretrained == True:\n",
    "        temp_model.load_weights(path_weights+'cell'+str(cell))\n",
    "    \n",
    "    # Training\n",
    "    loss_epoch = []\n",
    "    for epoch in range(nepochs):\n",
    "        if epoch == n_epochs_stim_freeze:\n",
    "            # Freeze the stimulus filter\n",
    "            temp_model.get_layer('stimulus_filter').trainable = False\n",
    "            temp_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Poisson())\n",
    "        loss_value, n_elt = 0, 0\n",
    "        \n",
    "        # Train the model\n",
    "        if n_parts_dataset == 1:\n",
    "            # Full dataset already in memory\n",
    "            for stim_train, coupl_train, self_train, refr_train, spikes_train in dataset_full:\n",
    "                n_elt += 1\n",
    "                loss_value += temp_model.train_on_batch([stim_train, coupl_train, self_train, refr_train], spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        else:\n",
    "            # Iterate over dataset slices\n",
    "            for ipart in range(n_parts_dataset):\n",
    "                dataset_slice = load_dataset(model='GLM', cell=cell, batch_size=batch_size, ipart=ipart, n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self)\n",
    "                for stim_train, coupl_train, self_train, refr_train, spikes_train in dataset_slice:\n",
    "                    n_elt += 1\n",
    "                    loss_value += temp_model.train_on_batch([stim_train, coupl_train, self_train, refr_train], spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        \n",
    "        # Print and store loss\n",
    "        print(f'Epoch: {epoch+1}, loss: {loss_value/n_elt}')\n",
    "        loss_epoch += [loss_value/n_elt]\n",
    "\n",
    "    # Store loss\n",
    "    losses[cell,:] = loss_epoch\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.plot(loss_epoch)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Store model\n",
    "    models += [temp_model]\n",
    "    \n",
    "# Clean temp data\n",
    "shutil.rmtree('./dataset_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_197138/3719462837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_weights\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'cell'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Save the weights and biases of the models\n",
    "path_weights = './model_weights/GLM/'\n",
    "\n",
    "if not os.path.exists(path_weights):\n",
    "    os.makedirs(path_weights)\n",
    "\n",
    "for cell in range(n_cells):\n",
    "    model = models[cell]\n",
    "    model.save_weights(path_weights+'cell'+str(cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract filters from the models\n",
    "\n",
    "path_filters = './model_filters/GLM/'\n",
    "\n",
    "stim_filter, bias_list, coupl_filters, self_filters, tau_r_list = [], [], [], [], []\n",
    "for cell in range(n_cells):\n",
    "    # Weights and biases\n",
    "    stimulus_coeffs, bias = models[cell].get_layer(name='stimulus_filter').weights\n",
    "    coupling_coeffs = models[cell].get_layer(name='coupling_filters').weights[0]\n",
    "    self_coeffs = models[cell].get_layer(name='self_filter').weights[0]\n",
    "\n",
    "    # Stimulus model\n",
    "    stimulus_filter = np.matmul(np.reshape(stimulus_coeffs.numpy(), [nx*ny, n_basis_stim]), stim_basis)\n",
    "    stim_filter += [stimulus_filter[:,:,np.newaxis, np.newaxis]]\n",
    "    \n",
    "    bias_list += [bias.numpy()]\n",
    "    \n",
    "    # Couplings\n",
    "    coupl_filters += [np.matmul(np.reshape(coupling_coeffs.numpy(), [n_cells-1, n_basis_coupl]), coupl_basis)]\n",
    "    \n",
    "    # Self coupling and refractory period\n",
    "    self_basis, tau_r = self_basis_gen(last_peak_self, streach_self, n_basis_self,\n",
    "                                   nt_integ_self, cell, spikes)\n",
    "    self_filters += [np.matmul(self_coeffs.numpy().transpose(), self_basis)]\n",
    "    tau_r_list += [tau_r]\n",
    "    \n",
    "if not os.path.exists(path_filters):\n",
    "    os.makedirs(path_filters)\n",
    "\n",
    "with open(path_filters+'filters.pkl', 'wb') as f:\n",
    "    pkl.dump((stim_filter, bias_list, coupl_filters, self_filters, tau_r_list), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LN model inference\n",
    "\n",
    "# Pretrained model\n",
    "load_pretrained = False\n",
    "path_weights = './model_weights/LN/'\n",
    "\n",
    "# Split the dataset in parts of size < to:\n",
    "max_size_dataslice = 2048 # in Mo\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "nepochs = 50\n",
    "\n",
    "# Storage\n",
    "models = []\n",
    "losses = np.zeros((n_cells, nepochs))\n",
    "\n",
    "# Parameters of the temporal bases\n",
    "first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim = 1, 170, 100, 6, 200\n",
    "# Stimulus basis\n",
    "stim_basis = raised_cosine_basis(first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim)\n",
    "\n",
    "# Build the training dataset\n",
    "n_parts_dataset = build_dataset(stimulus, spikes, max_size_dataslice, stim_basis, 'LN')\n",
    "\n",
    "for cell in range(n_cells):\n",
    "    # If the whole dataset fits in memory, load it once\n",
    "    if n_parts_dataset == 1:\n",
    "        dataset_full = load_dataset(model='LN', cell=cell, batch_size=batch_size, ipart=0)\n",
    "\n",
    "    # Create the model\n",
    "    n_basis_stim, nt_integ_stim = stim_basis.shape\n",
    "\n",
    "    temp_model = compiled_LN_model(nx=nx, ny=ny, n_basis_stim=n_basis_stim, l1_reg_stim=0, \n",
    "                                   l2_reg_stim=0, l2_lapl_reg_stim=2e-3, lapl_axis='all')\n",
    "\n",
    "    # Load pretrain model weights\n",
    "    if load_pretrained == True:\n",
    "        temp_model.load_weights(path_weights+'cell'+str(cell))\n",
    "    \n",
    "    # Training\n",
    "    loss_epoch = []\n",
    "    for epoch in range(nepochs):\n",
    "        loss_value, n_elt = 0, 0\n",
    "        \n",
    "        # Train the model\n",
    "        if n_parts_dataset == 1:\n",
    "            # Full dataset already in memory\n",
    "            for stim_train, spikes_train in dataset_full:\n",
    "                n_elt += 1\n",
    "                loss_value += temp_model.train_on_batch(stim_train, spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        else:\n",
    "            # Iterate over dataset slices\n",
    "            for ipart in range(n_parts_dataset):\n",
    "                dataset_slice = load_dataset(model='LN', cell=cell, batch_size=batch_size, ipart=ipart)\n",
    "                for stim_train, spikes_train in dataset_slice:\n",
    "                    n_elt += 1\n",
    "                    loss_value += temp_model.train_on_batch(stim_train, spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        \n",
    "        # Print and store loss\n",
    "        print(f'Epoch: {epoch+1}, loss: {loss_value/n_elt}')\n",
    "        loss_epoch += [loss_value/n_elt]\n",
    "\n",
    "    # Store loss\n",
    "    losses[cell,:] = loss_epoch\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.plot(loss_epoch)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Store model\n",
    "    models += [temp_model]\n",
    "    \n",
    "# Clean temp data\n",
    "shutil.rmtree('./dataset_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights of the models\n",
    "path_weights = './model_weights/LN/'\n",
    "\n",
    "if not os.path.exists(path_weights):\n",
    "    os.makedirs(path_weights)\n",
    "\n",
    "for cell in range(n_cells):\n",
    "    model = models[cell]\n",
    "    model.save_weights(path_weights+'cell'+str(cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract filters from the models\n",
    "\n",
    "path_filters = './model_filters/LN/'\n",
    "\n",
    "stim_filter, bias_list, coupl_filters, self_filters, tau_r_list = [], [], [], [], []\n",
    "for cell in range(n_cells):\n",
    "    # Weights and biases\n",
    "    stimulus_coeffs, bias = models[cell].get_layer(name='stimulus_filter').weights\n",
    "\n",
    "    # Stimulus model\n",
    "    stimulus_filter = np.matmul(np.reshape(stimulus_coeffs.numpy(), [nx*ny, n_basis_stim]), stim_basis)\n",
    "    stim_filter += [stimulus_filter[:,:,np.newaxis, np.newaxis]]\n",
    "    \n",
    "    bias_list += [bias.numpy()]\n",
    "    \n",
    "if not os.path.exists(path_filters):\n",
    "    os.makedirs(path_filters)\n",
    "\n",
    "with open(path_filters+'filters.pkl', 'wb') as f:\n",
    "    pkl.dump((stim_filter, bias_list), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

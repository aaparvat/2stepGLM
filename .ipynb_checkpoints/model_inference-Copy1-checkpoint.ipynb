{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import sys\n",
    "sys.path.append('./helpers/')\n",
    "from scipy.io import loadmat\n",
    "from data_manip import *\n",
    "from temporal_bases import *\n",
    "from compiled_models import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle5 as pkl\n",
    "import os\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data must be in the following format:\n",
    "# stimulus: (nx, ny, nt, n_rep) for n_rep movies of 2D stimuli of length nt, eventually n_rep can be 1\n",
    "# spikes: (n_cells, nt, n_rep) for n_cells neurons\n",
    "\n",
    "# Load training data\n",
    "# with open('./data/' + 'data_bars_OFF_unrepeated' + '.pkl', 'rb') as f:\n",
    "#     data_train = pkl.load(f)\n",
    "    \n",
    "# spikes = data_train['spikes'][:,:,:]\n",
    "# stimulus = data_train['stimulus'][:,:,:,:]\n",
    "# print(f'Training data:\\n',\n",
    "#     f'stimulus shape: {stimulus.shape}\\n',\n",
    "#       f'spikes shape: {spikes.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of stimulus: (144051,)\n",
      "[8.34060500e-03 1.66812100e-02 2.50218150e-02 ... 1.20145581e+03\n",
      " 1.20146415e+03 1.20147249e+03]\n",
      "Number of spikes for each of 4 neurons: 31528 21553 49954 43126\n"
     ]
    }
   ],
   "source": [
    "### ====  1. Load the raw data ============\n",
    "datadir = 'GLMspiketraintutorial_python/data_RGCs/'\n",
    "stim = np.squeeze(loadmat(f'{datadir}Stim.mat')['Stim']) # contains stimulus value at each frame\n",
    "stim_times = np.squeeze(loadmat(f'{datadir}stimtimes.mat')['stimtimes']) # contains time in seconds at each frame (120 Hz)\n",
    "all_spike_times = [np.squeeze(x) for x in np.squeeze(loadmat(f'{datadir}SpTimes.mat')['SpTimes'])] # time of spikes for 4 neurons (in units of stim frames)\n",
    "print(f'length of stimulus: {stim.shape}')\n",
    "print(stim_times)\n",
    "print(f'Number of spikes for each of 4 neurons: {\" \".join([str(x.size) for x in all_spike_times])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Loaded RGC data: cell 1\n",
      "Number of stim frames: 144051 (1201.5 minutes)\n",
      "Time bin size: 8.3 ms\n",
      "Number of spikes: 21553 (mean rate=17.9 Hz)\n",
      "Training perf (R^2): lin-gauss GLM, no offset: 0.08\n",
      "Training perf (R^2): lin-gauss GLM, w/ offset: 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/opt/conda/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03711846 0.03988954 0.0632193  ... 0.49095793 0.38238551 0.09204973]\n",
      "\n",
      " empirical single-spike information:\n",
      " ---------------------- \n",
      "exp-GLM: 1.34 bits/sp\n",
      " np-GLM: 1.44 bits/sp\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1507/394909701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;31m# Or uncomment this line to use the non-parametric nonlinearity instead:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mf_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_stim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfnlin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpGLM_const\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdesign_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miiplot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mpGLM_filt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# firing rate in each bin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m# Simulate spikes using draws from a Bernoulli (coin flipping) process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/interpolate/polyint.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[1;32m     77\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/interpolate/interpolate.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, x_new)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;31m#    The behavior is set by the bounds_error variable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0my_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extrapolate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mbelow_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabove_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/scipy/interpolate/interpolate.py\u001b[0m in \u001b[0;36m_call_nearest\u001b[0;34m(self, x_new)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m#    Note: use side='left' (right) to searchsorted() to define the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m#    halfway point to be nearest to the left (right) neighbor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mx_new_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_bds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_side\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;31m# 3. Clip x_new_indices so that they are within the range of x indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msearchsorted\u001b[0;34m(a, v, side, sorter)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_searchsorted_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training data must be in the following format:\n",
    "# spikes: (n_cells, nt, n_rep)\n",
    "# Load training data\n",
    "# let's work with the third cell for now\n",
    "\n",
    "cell_idx = 1\n",
    "spike_times = all_spike_times[cell_idx]\n",
    "\n",
    "# Print out some basic info\n",
    "dt_stim = stim_times[1] - stim_times[0] # time bin size\n",
    "refresh_rate = 1/dt_stim # refresh rate of the monitor\n",
    "num_time_bins = stim.size # number of time bins in stimulus\n",
    "num_spikes = spike_times.size # number of spikes\n",
    "print('--------------------------')\n",
    "print(f'Loaded RGC data: cell {cell_idx}')\n",
    "print(f'Number of stim frames: {num_time_bins} ({num_time_bins*dt_stim:.1f} minutes)')\n",
    "print(f'Time bin size: {dt_stim*1000:.1f} ms')\n",
    "print(f'Number of spikes: {num_spikes} (mean rate={num_spikes/num_time_bins*refresh_rate:.1f} Hz)')\n",
    "### ==== 2. Bin the spike train ===== \n",
    "\n",
    "# For now we will assume we want to use the same time bin size as the time\n",
    "# bins used for the stimulus. Later, though, we'll wish to vary this.\n",
    "\n",
    "spikes_bin_centers = np.arange(num_time_bins+1) * dt_stim # centers of bins for applying to spike train\n",
    "spikes_binned,_ = np.histogram(spike_times, spikes_bin_centers)\n",
    "### ==== 3. Build the design matrix: slow version ======\n",
    "# This is a necessary step before we can fit the model: assemble a matrix\n",
    "# that contains the relevant regressors for each time bin of the response,\n",
    "# known as a design matrix.  Each row of this matrix contains the relevant\n",
    "# stimulus chunk for predicting the spike count at a given time bin\n",
    "\n",
    "# Set the number of time bins of stimulus to use for predicting spikes\n",
    "ntfilt = 25    # Try varying this, to see how performance changes!\n",
    "\n",
    "# Build the design matrix: Slow version\n",
    "padded_stim = np.hstack((np.zeros((ntfilt-1)), stim)) # pad early bins of stimulus with zero\n",
    "design_mat = np.zeros((num_time_bins,ntfilt))\n",
    "for j in np.arange(num_time_bins):\n",
    "    design_mat[j] = padded_stim[j:j+ntfilt] # grab last 'nkt' bins of stmiulus and insert into this row\n",
    "    \n",
    "\n",
    "\n",
    "# Notice it has a structure where every row is a shifted copy of the row\n",
    "# above, which comes from the fact that for each time bin of response,\n",
    "# we're grabbing the preceding 'nkt' bins of stimulus as predictor\n",
    "from numpy.linalg import inv, norm\n",
    "sta = (design_mat.T @ spikes_binned)/num_spikes\n",
    "wsta = inv(design_mat.T @ design_mat) @ sta * num_spikes\n",
    "# this is just the least-squares regression formula!\n",
    "sppred_lgGLM = design_mat @ wsta\n",
    "design_mat_offset = np.hstack((np.ones((num_time_bins,1)), design_mat))     # just add a column of ones\n",
    "\n",
    "### Compute whitened STA\n",
    "wsta_offset = inv(design_mat_offset.T @ design_mat_offset) @ (design_mat_offset.T @ spikes_binned)  # this is just the LS regression formula\n",
    "const = wsta_offset[0]   # the additive constant\n",
    "wsta_offset = wsta_offset[1:]  # the linear filter part\n",
    "\n",
    "### Now redo prediction (with offset)\n",
    "sppred_lgGLM_offset = const + design_mat @ wsta_offset\n",
    "mse1 = np.mean((spikes_binned-sppred_lgGLM)**2)   # mean squared error, GLM no offset\n",
    "mse2 = np.mean((spikes_binned-sppred_lgGLM_offset)**2)  # mean squared error, with offset\n",
    "rss = np.mean((spikes_binned-np.mean(spikes_binned))**2)    # squared error of spike train\n",
    "print('Training perf (R^2): lin-gauss GLM, no offset: {:.2f}'.format(1-mse1/rss))\n",
    "print('Training perf (R^2): lin-gauss GLM, w/ offset: {:.2f}'.format(1-mse2/rss))\n",
    "## ======  5. Poisson GLM ====================\n",
    "\n",
    "# Let's finally move on to the LNP / Poisson GLM!\n",
    "\n",
    "# Package available for download from\n",
    "# https://www.statsmodels.org/stable/install.html\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### This is super-easy if we rely on built-in GLM fitting code\n",
    "glm_poisson_exp = sm.GLM(endog=spikes_binned, exog=design_mat_offset,\n",
    "                         family=sm.families.Poisson())\n",
    "\n",
    "pGLM_results = glm_poisson_exp.fit(max_iter=100, tol=1e-6, tol_criterion='params')\n",
    "\n",
    "\n",
    "# pGLM_const = glm_poisson_exp[-1].fit_['beta0'] # constant (\"dc term)\")\n",
    "pGLM_const = pGLM_results.params[0]\n",
    "pGLM_filt = pGLM_results.params[1:] # stimulus filter\n",
    "\n",
    "# The 'GLM' function can fit a GLM for us. Here we have specified that\n",
    "# we want the noise model to be Poisson. The default setting for the link\n",
    "# function (the inverse of the nonlinearity) is 'log', so default\n",
    "# nonlinearity is 'exp').  \n",
    "\n",
    "### Compute predicted spike rate on training data\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat @ pGLM_filt)\n",
    "# equivalent to if we had just written np.exp(design_mat_offset @ glm_poisson_exp)/dt_stim\n",
    "print(rate_pred_pGLM)\n",
    "# The above fitting code assumes a GLM with an exponential nonlinearity\n",
    "# (i.e., governing the mapping from filter output to instantaneous spike\n",
    "# rate). We might wish to examine the adequacy of that assumption and make\n",
    "# a \"nonparametric\" estimate of the nonlinearity using a more flexible\n",
    "# class of functions.\n",
    "\n",
    "# Let's use the family of piece-wise constant functions, which results in a\n",
    "# very simple estimation procedure:\n",
    "# 1. Bin the filter outputs\n",
    "# 2. In each bin, compute the fraction of stimuli elicted spikes\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# number of bins for parametrizing the nonlinearity f. (Try varying this!) \n",
    "num_fbins = 25\n",
    "\n",
    "# compute filtered stimulus\n",
    "raw_filter_output = pGLM_const + design_mat @ pGLM_filt\n",
    "\n",
    "# bin filter output and get bin index for each filtered stimulus\n",
    "counts,bin_edges = np.histogram(raw_filter_output,num_fbins);\n",
    "bin_idx = np.digitize(raw_filter_output, bins=bin_edges) - 1\n",
    "fx = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2 # use bin centers for x positions\n",
    "\n",
    "# now compute mean spike count in each bin\n",
    "fy = np.zeros(num_fbins) # y values for nonlinearity\n",
    "for jj in np.arange(num_fbins):\n",
    "    fy[jj] = np.mean(spikes_binned[bin_idx==jj])\n",
    "fy = fy/dt_stim # divide by bin size to get units of sp/s;\n",
    "\n",
    "# Scipy has a handy class that embeds these approximations into an interpolating function\n",
    "fnlin = interp1d(fx,fy,kind='nearest', bounds_error=False, fill_value='extrapolate')\n",
    "### ======= 7. Quantifying performance: log-likelihood =======\n",
    "\n",
    "# Lastly, compute log-likelihood for the Poisson GLMs we've used so far and\n",
    "# compare performance.\n",
    "\n",
    "# LOG-LIKELIHOOD (this is what glmfit maximizes when fitting the GLM):\n",
    "# --------------\n",
    "# Let s be the spike count in a bin and r is the predicted spike rate\n",
    "# (known as \"conditional intensity\") in units of spikes/bin, then we have:   \n",
    "#\n",
    "#        Poisson likelihood:      P(s|r) = r^s/s! exp(-r)  \n",
    "#     giving log-likelihood:  log P(s|r) =  s log r - r   \n",
    "#\n",
    "# (where we have ignored the -log s! term because it is independent of the\n",
    "# parameters). The total log-likelihood is the summed log-likelihood over\n",
    "# time bins in the experiment.\n",
    "\n",
    "# 1. for GLM with exponential nonlinearity\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat@pGLM_filt)# rate under exp nonlinearity\n",
    "LL_expGLM = spikes_binned.T @ np.log(rate_pred_pGLM) - np.sum(rate_pred_pGLM)\n",
    "\n",
    "# 2. for GLM with non-parametric nonlinearity\n",
    "rate_pred_pGLMnp = dt_stim * fnlin(pGLM_const + design_mat @ pGLM_filt) # rate under nonpar nonlinearity\n",
    "LL_npGLM = spikes_binned[spikes_binned>0].T @ np.log(rate_pred_pGLMnp[spikes_binned>0]) - np.sum(rate_pred_pGLMnp)\n",
    "\n",
    "# Now compute the rate under \"homogeneous\" Poisson model that assumes a\n",
    "# constant firing rate with the correct mean spike count.\n",
    "rate_pred_const = num_spikes/num_time_bins  # mean number of spikes / bin\n",
    "LL0 = num_spikes*np.log(rate_pred_const) - num_time_bins*rate_pred_const\n",
    "# Single-spike information:\n",
    "# ------------------------\n",
    "# The difference of the loglikelihood and homogeneous-Poisson\n",
    "# loglikelihood, normalized by the number of spikes, gives us an intuitive\n",
    "# way to compare log-likelihoods in units of bits / spike.  This is a\n",
    "# quantity known as the (empirical) single-spike information.\n",
    "# [See Brenner et al, \"Synergy in a Neural Code\", Neural Comp 2000].\n",
    "# You can think of this as the number of bits (number of yes/no questions\n",
    "# that we can answer) about the times of spikes when we know the spike rate\n",
    "# output by the model, compared to when we only know the (constant) mean\n",
    "# spike rate. \n",
    "\n",
    "SSinfo_expGLM = (LL_expGLM - LL0)/num_spikes/np.log(2)\n",
    "SSinfo_npGLM = (LL_npGLM - LL0)/num_spikes/np.log(2)\n",
    "# (if we don't divide by log 2 we get it in nats)\n",
    "\n",
    "print('\\n empirical single-spike information:\\n ---------------------- ')\n",
    "print(f'exp-GLM: {SSinfo_expGLM:.2f} bits/sp')\n",
    "print(f' np-GLM: {SSinfo_npGLM:.2f} bits/sp')\n",
    "AIC_expGLM = -2*LL_expGLM + 2*(1+ntfilt);\n",
    "AIC_npGLM = -2*LL_npGLM + 2*(1+ntfilt+num_fbins)\n",
    "### ========= 9. Simulating the GLM / making a raster plot =========\n",
    "\n",
    "# Lastly, let's simulate the response of the GLM to a repeated stimulus and\n",
    "# make raster plots \n",
    "\n",
    "iiplot = np.arange(60) # time bins of stimulus to use\n",
    "ttplot = iiplot*dt_stim # time indices for these stimuli\n",
    "stim_repeat = stim[iiplot] # repeat stimulus \n",
    "num_repeats = 50;  # number of repeats\n",
    "#f_rate = np.exp(pGLM_const + design_mat[iiplot,:] @ pGLM_filt)# firing rate in each bin\n",
    "\n",
    "# Or uncomment this line to use the non-parametric nonlinearity instead:\n",
    "f_rate = dt_stim*fnlin(pGLM_const+design_mat[iiplot,:]@pGLM_filt) # firing rate in each bin\n",
    "\n",
    "# Simulate spikes using draws from a Bernoulli (coin flipping) process\n",
    "spike_counts1 = np.random.poisson(np.tile(f_rate.T,[num_repeats,1])) # sample spike counts for each time bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's work with the third cell for now\n",
    "cell_idx = 0\n",
    "spike_times = all_spike_times[cell_idx]\n",
    "\n",
    "# Print out some basic info\n",
    "dt_stim = stim_times[1] - stim_times[0] # time bin size\n",
    "refresh_rate = 1/dt_stim # refresh rate of the monitor\n",
    "num_time_bins = stim.size # number of time bins in stimulus\n",
    "num_spikes = spike_times.size # number of spikes\n",
    "print('--------------------------')\n",
    "print(f'Loaded RGC data: cell {cell_idx}')\n",
    "print(f'Number of stim frames: {num_time_bins} ({num_time_bins*dt_stim:.1f} minutes)')\n",
    "print(f'Time bin size: {dt_stim*1000:.1f} ms')\n",
    "print(f'Number of spikes: {num_spikes} (mean rate={num_spikes/num_time_bins*refresh_rate:.1f} Hz)')\n",
    "\n",
    "### ==== 3. Build the design matrix: slow version ======\n",
    "# This is a necessary step before we can fit the model: assemble a matrix\n",
    "# that contains the relevant regressors for each time bin of the response,\n",
    "# known as a design matrix.  Each row of this matrix contains the relevant\n",
    "# stimulus chunk for predicting the spike count at a given time bin\n",
    "\n",
    "# Set the number of time bins of stimulus to use for predicting spikes\n",
    "ntfilt = 25    # Try varying this, to see how performance changes!\n",
    "\n",
    "# Build the design matrix: Slow version\n",
    "padded_stim = np.hstack((np.zeros((ntfilt-1)), stim)) # pad early bins of stimulus with zero\n",
    "design_mat = np.zeros((num_time_bins,ntfilt))\n",
    "for j in np.arange(num_time_bins):\n",
    "    design_mat[j] = padded_stim[j:j+ntfilt] # grab last 'nkt' bins of stmiulus and insert into this row\n",
    "\n",
    "from numpy.linalg import inv, norm\n",
    "sta = (design_mat.T @ spikes_binned)/num_spikes\n",
    "wsta = inv(design_mat.T @ design_mat) @ sta * num_spikes\n",
    "# this is just the least-squares regression formula!\n",
    "sppred_lgGLM = design_mat @ wsta\n",
    "design_mat_offset = np.hstack((np.ones((num_time_bins,1)), design_mat))     # just add a column of ones\n",
    "\n",
    "### Compute whitened STA\n",
    "wsta_offset = inv(design_mat_offset.T @ design_mat_offset) @ (design_mat_offset.T @ spikes_binned)  # this is just the LS regression formula\n",
    "const = wsta_offset[0]   # the additive constant\n",
    "wsta_offset = wsta_offset[1:]  # the linear filter part\n",
    "\n",
    "### Now redo prediction (with offset)\n",
    "sppred_lgGLM_offset = const + design_mat @ wsta_offset\n",
    "mse1 = np.mean((spikes_binned-sppred_lgGLM)**2)   # mean squared error, GLM no offset\n",
    "mse2 = np.mean((spikes_binned-sppred_lgGLM_offset)**2)  # mean squared error, with offset\n",
    "rss = np.mean((spikes_binned-np.mean(spikes_binned))**2)    # squared error of spike train\n",
    "print('Training perf (R^2): lin-gauss GLM, no offset: {:.2f}'.format(1-mse1/rss))\n",
    "print('Training perf (R^2): lin-gauss GLM, w/ offset: {:.2f}'.format(1-mse2/rss))\n",
    "## ======  5. Poisson GLM ====================\n",
    "\n",
    "# Let's finally move on to the LNP / Poisson GLM!\n",
    "\n",
    "# Package available for download from\n",
    "# https://www.statsmodels.org/stable/install.html\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### This is super-easy if we rely on built-in GLM fitting code\n",
    "glm_poisson_exp = sm.GLM(endog=spikes_binned, exog=design_mat_offset,\n",
    "                         family=sm.families.Poisson())\n",
    "\n",
    "pGLM_results = glm_poisson_exp.fit(max_iter=100, tol=1e-6, tol_criterion='params')\n",
    "\n",
    "\n",
    "# pGLM_const = glm_poisson_exp[-1].fit_['beta0'] # constant (\"dc term)\")\n",
    "pGLM_const = pGLM_results.params[0]\n",
    "pGLM_filt = pGLM_results.params[1:] # stimulus filter\n",
    "\n",
    "# The 'GLM' function can fit a GLM for us. Here we have specified that\n",
    "# we want the noise model to be Poisson. The default setting for the link\n",
    "# function (the inverse of the nonlinearity) is 'log', so default\n",
    "# nonlinearity is 'exp').  \n",
    "\n",
    "### Compute predicted spike rate on training data\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat @ pGLM_filt)\n",
    "# equivalent to if we had just written np.exp(design_mat_offset @ glm_poisson_exp)/dt_stim\n",
    "print(rate_pred_pGLM)\n",
    "#The above fitting code assumes a GLM with an exponential nonlinearity\n",
    "# (i.e., governing the mapping from filter output to instantaneous spike\n",
    "# rate). We might wish to examine the adequacy of that assumption and make\n",
    "# a \"nonparametric\" estimate of the nonlinearity using a more flexible\n",
    "# class of functions.\n",
    "\n",
    "# Let's use the family of piece-wise constant functions, which results in a\n",
    "# very simple estimation procedure:\n",
    "# 1. Bin the filter outputs\n",
    "# 2. In each bin, compute the fraction of stimuli elicted spikes\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# number of bins for parametrizing the nonlinearity f. (Try varying this!) \n",
    "num_fbins = 25\n",
    "\n",
    "# compute filtered stimulus\n",
    "raw_filter_output = pGLM_const + design_mat @ pGLM_filt\n",
    "\n",
    "# bin filter output and get bin index for each filtered stimulus\n",
    "counts,bin_edges = np.histogram(raw_filter_output,num_fbins);\n",
    "bin_idx = np.digitize(raw_filter_output, bins=bin_edges) - 1\n",
    "fx = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2 # use bin centers for x positions\n",
    "\n",
    "# now compute mean spike count in each bin\n",
    "fy = np.zeros(num_fbins) # y values for nonlinearity\n",
    "for jj in np.arange(num_fbins):\n",
    "    fy[jj] = np.mean(spikes_binned[bin_idx==jj])\n",
    "fy = fy/dt_stim # divide by bin size to get units of sp/s;\n",
    "\n",
    "# Scipy has a handy class that embeds these approximations into an interpolating function\n",
    "fnlin = interp1d(fx,fy,kind='nearest', bounds_error=False, fill_value='extrapolate')\n",
    "### ======= 7. Quantifying performance: log-likelihood =======\n",
    "\n",
    "# Lastly, compute log-likelihood for the Poisson GLMs we've used so far and\n",
    "# compare performance.\n",
    "\n",
    "# LOG-LIKELIHOOD (this is what glmfit maximizes when fitting the GLM):\n",
    "# --------------\n",
    "# Let s be the spike count in a bin and r is the predicted spike rate\n",
    "# (known as \"conditional intensity\") in units of spikes/bin, then we have:   \n",
    "#\n",
    "#        Poisson likelihood:      P(s|r) = r^s/s! exp(-r)  \n",
    "#     giving log-likelihood:  log P(s|r) =  s log r - r   \n",
    "#\n",
    "# (where we have ignored the -log s! term because it is independent of the\n",
    "# parameters). The total log-likelihood is the summed log-likelihood over\n",
    "# time bins in the experiment.\n",
    "\n",
    "# 1. for GLM with exponential nonlinearity\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat@pGLM_filt)# rate under exp nonlinearity\n",
    "LL_expGLM = spikes_binned.T @ np.log(rate_pred_pGLM) - np.sum(rate_pred_pGLM)\n",
    "\n",
    "# 2. for GLM with non-parametric nonlinearity\n",
    "rate_pred_pGLMnp = dt_stim * fnlin(pGLM_const + design_mat @ pGLM_filt) # rate under nonpar nonlinearity\n",
    "LL_npGLM = spikes_binned[spikes_binned>0].T @ np.log(rate_pred_pGLMnp[spikes_binned>0]) - np.sum(rate_pred_pGLMnp)\n",
    "\n",
    "# Now compute the rate under \"homogeneous\" Poisson model that assumes a\n",
    "# constant firing rate with the correct mean spike count.\n",
    "rate_pred_const = num_spikes/num_time_bins  # mean number of spikes / bin\n",
    "LL0 = num_spikes*np.log(rate_pred_const) - num_time_bins*rate_pred_const\n",
    "# Single-spike information:\n",
    "# ------------------------\n",
    "# The difference of the loglikelihood and homogeneous-Poisson\n",
    "# loglikelihood, normalized by the number of spikes, gives us an intuitive\n",
    "# way to compare log-likelihoods in units of bits / spike.  This is a\n",
    "# quantity known as the (empirical) single-spike information.\n",
    "# [See Brenner et al, \"Synergy in a Neural Code\", Neural Comp 2000].\n",
    "# You can think of this as the number of bits (number of yes/no questions\n",
    "# that we can answer) about the times of spikes when we know the spike rate\n",
    "# output by the model, compared to when we only know the (constant) mean\n",
    "# spike rate. \n",
    "\n",
    "SSinfo_expGLM = (LL_expGLM - LL0)/num_spikes/np.log(2)\n",
    "SSinfo_npGLM = (LL_npGLM - LL0)/num_spikes/np.log(2)\n",
    "# (if we don't divide by log 2 we get it in nats)\n",
    "\n",
    "print('\\n empirical single-spike information:\\n ---------------------- ')\n",
    "print(f'exp-GLM: {SSinfo_expGLM:.2f} bits/sp')\n",
    "print(f' np-GLM: {SSinfo_npGLM:.2f} bits/sp')\n",
    "AIC_expGLM = -2*LL_expGLM + 2*(1+ntfilt);\n",
    "AIC_npGLM = -2*LL_npGLM + 2*(1+ntfilt+num_fbins)\n",
    "### ========= 9. Simulating the GLM / making a raster plot =========\n",
    "\n",
    "# Lastly, let's simulate the response of the GLM to a repeated stimulus and\n",
    "# make raster plots \n",
    "\n",
    "iiplot = np.arange(60) # time bins of stimulus to use\n",
    "ttplot = iiplot*dt_stim # time indices for these stimuli\n",
    "stim_repeat = stim[iiplot] # repeat stimulus \n",
    "num_repeats = 50;  # number of repeats\n",
    "#f_rate = np.exp(pGLM_const + design_mat[iiplot,:] @ pGLM_filt)# firing rate in each bin\n",
    "\n",
    "# Or uncomment this line to use the non-parametric nonlinearity instead:\n",
    "f_rate = dt_stim*fnlin(pGLM_const+design_mat[iiplot,:]@pGLM_filt) # firing rate in each bin\n",
    "\n",
    "# Simulate spikes using draws from a Bernoulli (coin flipping) process\n",
    "spike_counts0 = np.random.poisson(np.tile(f_rate.T,[num_repeats,1])) # sample spike counts for each time bin\n",
    "plt.eventplot(spike_counts0)\n",
    "plt.ylabel('repeat #')\n",
    "plt.xlabel('time (s)')\n",
    "plt.title('GLM spike trains')\n",
    "print(spike_counts0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's work with the third cell for now\n",
    "cell_idx = 2\n",
    "spike_times = all_spike_times[cell_idx]\n",
    "\n",
    "# Print out some basic info\n",
    "dt_stim = stim_times[1] - stim_times[0] # time bin size\n",
    "refresh_rate = 1/dt_stim # refresh rate of the monitor\n",
    "num_time_bins = stim.size # number of time bins in stimulus\n",
    "num_spikes = spike_times.size # number of spikes\n",
    "print('--------------------------')\n",
    "print(f'Loaded RGC data: cell {cell_idx}')\n",
    "print(f'Number of stim frames: {num_time_bins} ({num_time_bins*dt_stim:.1f} minutes)')\n",
    "print(f'Time bin size: {dt_stim*1000:.1f} ms')\n",
    "print(f'Number of spikes: {num_spikes} (mean rate={num_spikes/num_time_bins*refresh_rate:.1f} Hz)')\n",
    "\n",
    "### ==== 3. Build the design matrix: slow version ======\n",
    "# This is a necessary step before we can fit the model: assemble a matrix\n",
    "# that contains the relevant regressors for each time bin of the response,\n",
    "# known as a design matrix.  Each row of this matrix contains the relevant\n",
    "# stimulus chunk for predicting the spike count at a given time bin\n",
    "\n",
    "# Set the number of time bins of stimulus to use for predicting spikes\n",
    "ntfilt = 25    # Try varying this, to see how performance changes!\n",
    "\n",
    "# Build the design matrix: Slow version\n",
    "padded_stim = np.hstack((np.zeros((ntfilt-1)), stim)) # pad early bins of stimulus with zero\n",
    "design_mat = np.zeros((num_time_bins,ntfilt))\n",
    "for j in np.arange(num_time_bins):\n",
    "    design_mat[j] = padded_stim[j:j+ntfilt] # grab last 'nkt' bins of stmiulus and insert into this row\n",
    "\n",
    "from numpy.linalg import inv, norm\n",
    "sta = (design_mat.T @ spikes_binned)/num_spikes\n",
    "wsta = inv(design_mat.T @ design_mat) @ sta * num_spikes\n",
    "# this is just the least-squares regression formula!\n",
    "sppred_lgGLM = design_mat @ wsta\n",
    "design_mat_offset = np.hstack((np.ones((num_time_bins,1)), design_mat))     # just add a column of ones\n",
    "\n",
    "### Compute whitened STA\n",
    "wsta_offset = inv(design_mat_offset.T @ design_mat_offset) @ (design_mat_offset.T @ spikes_binned)  # this is just the LS regression formula\n",
    "const = wsta_offset[0]   # the additive constant\n",
    "wsta_offset = wsta_offset[1:]  # the linear filter part\n",
    "\n",
    "### Now redo prediction (with offset)\n",
    "sppred_lgGLM_offset = const + design_mat @ wsta_offset\n",
    "mse1 = np.mean((spikes_binned-sppred_lgGLM)**2)   # mean squared error, GLM no offset\n",
    "mse2 = np.mean((spikes_binned-sppred_lgGLM_offset)**2)  # mean squared error, with offset\n",
    "rss = np.mean((spikes_binned-np.mean(spikes_binned))**2)    # squared error of spike train\n",
    "print('Training perf (R^2): lin-gauss GLM, no offset: {:.2f}'.format(1-mse1/rss))\n",
    "print('Training perf (R^2): lin-gauss GLM, w/ offset: {:.2f}'.format(1-mse2/rss))\n",
    "## ======  5. Poisson GLM ====================\n",
    "\n",
    "# Let's finally move on to the LNP / Poisson GLM!\n",
    "\n",
    "# Package available for download from\n",
    "# https://www.statsmodels.org/stable/install.html\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### This is super-easy if we rely on built-in GLM fitting code\n",
    "glm_poisson_exp = sm.GLM(endog=spikes_binned, exog=design_mat_offset,\n",
    "                         family=sm.families.Poisson())\n",
    "\n",
    "pGLM_results = glm_poisson_exp.fit(max_iter=100, tol=1e-6, tol_criterion='params')\n",
    "\n",
    "\n",
    "# pGLM_const = glm_poisson_exp[-1].fit_['beta0'] # constant (\"dc term)\")\n",
    "pGLM_const = pGLM_results.params[0]\n",
    "pGLM_filt = pGLM_results.params[1:] # stimulus filter\n",
    "\n",
    "# The 'GLM' function can fit a GLM for us. Here we have specified that\n",
    "# we want the noise model to be Poisson. The default setting for the link\n",
    "# function (the inverse of the nonlinearity) is 'log', so default\n",
    "# nonlinearity is 'exp').  \n",
    "\n",
    "### Compute predicted spike rate on training data\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat @ pGLM_filt)\n",
    "# equivalent to if we had just written np.exp(design_mat_offset @ glm_poisson_exp)/dt_stim\n",
    "print(rate_pred_pGLM)\n",
    "#The above fitting code assumes a GLM with an exponential nonlinearity\n",
    "# (i.e., governing the mapping from filter output to instantaneous spike\n",
    "# rate). We might wish to examine the adequacy of that assumption and make\n",
    "# a \"nonparametric\" estimate of the nonlinearity using a more flexible\n",
    "# class of functions.\n",
    "\n",
    "# Let's use the family of piece-wise constant functions, which results in a\n",
    "# very simple estimation procedure:\n",
    "# 1. Bin the filter outputs\n",
    "# 2. In each bin, compute the fraction of stimuli elicted spikes\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# number of bins for parametrizing the nonlinearity f. (Try varying this!) \n",
    "num_fbins = 25\n",
    "\n",
    "# compute filtered stimulus\n",
    "raw_filter_output = pGLM_const + design_mat @ pGLM_filt\n",
    "\n",
    "# bin filter output and get bin index for each filtered stimulus\n",
    "counts,bin_edges = np.histogram(raw_filter_output,num_fbins);\n",
    "bin_idx = np.digitize(raw_filter_output, bins=bin_edges) - 1\n",
    "fx = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2 # use bin centers for x positions\n",
    "\n",
    "# now compute mean spike count in each bin\n",
    "fy = np.zeros(num_fbins) # y values for nonlinearity\n",
    "for jj in np.arange(num_fbins):\n",
    "    fy[jj] = np.mean(spikes_binned[bin_idx==jj])\n",
    "fy = fy/dt_stim # divide by bin size to get units of sp/s;\n",
    "\n",
    "# Scipy has a handy class that embeds these approximations into an interpolating function\n",
    "fnlin = interp1d(fx,fy,kind='nearest', bounds_error=False, fill_value='extrapolate')\n",
    "### ======= 7. Quantifying performance: log-likelihood =======\n",
    "\n",
    "# Lastly, compute log-likelihood for the Poisson GLMs we've used so far and\n",
    "# compare performance.\n",
    "\n",
    "# LOG-LIKELIHOOD (this is what glmfit maximizes when fitting the GLM):\n",
    "# --------------\n",
    "# Let s be the spike count in a bin and r is the predicted spike rate\n",
    "# (known as \"conditional intensity\") in units of spikes/bin, then we have:   \n",
    "#\n",
    "#        Poisson likelihood:      P(s|r) = r^s/s! exp(-r)  \n",
    "#     giving log-likelihood:  log P(s|r) =  s log r - r   \n",
    "#\n",
    "# (where we have ignored the -log s! term because it is independent of the\n",
    "# parameters). The total log-likelihood is the summed log-likelihood over\n",
    "# time bins in the experiment.\n",
    "\n",
    "# 1. for GLM with exponential nonlinearity\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat@pGLM_filt)# rate under exp nonlinearity\n",
    "LL_expGLM = spikes_binned.T @ np.log(rate_pred_pGLM) - np.sum(rate_pred_pGLM)\n",
    "\n",
    "# 2. for GLM with non-parametric nonlinearity\n",
    "rate_pred_pGLMnp = dt_stim * fnlin(pGLM_const + design_mat @ pGLM_filt) # rate under nonpar nonlinearity\n",
    "LL_npGLM = spikes_binned[spikes_binned>0].T @ np.log(rate_pred_pGLMnp[spikes_binned>0]) - np.sum(rate_pred_pGLMnp)\n",
    "\n",
    "# Now compute the rate under \"homogeneous\" Poisson model that assumes a\n",
    "# constant firing rate with the correct mean spike count.\n",
    "rate_pred_const = num_spikes/num_time_bins  # mean number of spikes / bin\n",
    "LL0 = num_spikes*np.log(rate_pred_const) - num_time_bins*rate_pred_const\n",
    "# Single-spike information:\n",
    "# ------------------------\n",
    "# The difference of the loglikelihood and homogeneous-Poisson\n",
    "# loglikelihood, normalized by the number of spikes, gives us an intuitive\n",
    "# way to compare log-likelihoods in units of bits / spike.  This is a\n",
    "# quantity known as the (empirical) single-spike information.\n",
    "# [See Brenner et al, \"Synergy in a Neural Code\", Neural Comp 2000].\n",
    "# You can think of this as the number of bits (number of yes/no questions\n",
    "# that we can answer) about the times of spikes when we know the spike rate\n",
    "# output by the model, compared to when we only know the (constant) mean\n",
    "# spike rate. \n",
    "\n",
    "SSinfo_expGLM = (LL_expGLM - LL0)/num_spikes/np.log(2)\n",
    "SSinfo_npGLM = (LL_npGLM - LL0)/num_spikes/np.log(2)\n",
    "# (if we don't divide by log 2 we get it in nats)\n",
    "\n",
    "print('\\n empirical single-spike information:\\n ---------------------- ')\n",
    "print(f'exp-GLM: {SSinfo_expGLM:.2f} bits/sp')\n",
    "print(f' np-GLM: {SSinfo_npGLM:.2f} bits/sp')\n",
    "AIC_expGLM = -2*LL_expGLM + 2*(1+ntfilt);\n",
    "AIC_npGLM = -2*LL_npGLM + 2*(1+ntfilt+num_fbins)\n",
    "### ========= 9. Simulating the GLM / making a raster plot =========\n",
    "\n",
    "# Lastly, let's simulate the response of the GLM to a repeated stimulus and\n",
    "# make raster plots \n",
    "\n",
    "iiplot = np.arange(60) # time bins of stimulus to use\n",
    "ttplot = iiplot*dt_stim # time indices for these stimuli\n",
    "stim_repeat = stim[iiplot] # repeat stimulus \n",
    "num_repeats = 50;  # number of repeats\n",
    "#f_rate = np.exp(pGLM_const + design_mat[iiplot,:] @ pGLM_filt)# firing rate in each bin\n",
    "\n",
    "# Or uncomment this line to use the non-parametric nonlinearity instead:\n",
    "f_rate = dt_stim*fnlin(pGLM_const+design_mat[iiplot,:]@pGLM_filt) # firing rate in each bin\n",
    "\n",
    "# Simulate spikes using draws from a Bernoulli (coin flipping) process\n",
    "spike_counts2 = np.random.poisson(np.tile(f_rate.T,[num_repeats,1])) # sample spike counts for each time bin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's work with the third cell for now\n",
    "cell_idx = 3\n",
    "spike_times = all_spike_times[cell_idx]\n",
    "\n",
    "# Print out some basic info\n",
    "dt_stim = stim_times[1] - stim_times[0] # time bin size\n",
    "refresh_rate = 1/dt_stim # refresh rate of the monitor\n",
    "num_time_bins = stim.size # number of time bins in stimulus\n",
    "num_spikes = spike_times.size # number of spikes\n",
    "print('--------------------------')\n",
    "print(f'Loaded RGC data: cell {cell_idx}')\n",
    "print(f'Number of stim frames: {num_time_bins} ({num_time_bins*dt_stim:.1f} minutes)')\n",
    "print(f'Time bin size: {dt_stim*1000:.1f} ms')\n",
    "print(f'Number of spikes: {num_spikes} (mean rate={num_spikes/num_time_bins*refresh_rate:.1f} Hz)')\n",
    "\n",
    "spike_times_plot = spike_times[(spike_times>=ttplot[0]) & (spike_times<ttplot[-1])]\n",
    "spikes_bin_centers = np.arange(num_time_bins+1) * dt_stim # centers of bins for applying to spike train\n",
    "spikes_binned,_ = np.histogram(spike_times, spikes_bin_centers)\n",
    "### ==== 3. Build the design matrix: slow version ======\n",
    "# This is a necessary step before we can fit the model: assemble a matrix\n",
    "# that contains the relevant regressors for each time bin of the response,\n",
    "# known as a design matrix.  Each row of this matrix contains the relevant\n",
    "# stimulus chunk for predicting the spike count at a given time bin\n",
    "\n",
    "# Set the number of time bins of stimulus to use for predicting spikes\n",
    "ntfilt = 25    # Try varying this, to see how performance changes!\n",
    "\n",
    "# Build the design matrix: Slow version\n",
    "padded_stim = np.hstack((np.zeros((ntfilt-1)), stim)) # pad early bins of stimulus with zero\n",
    "design_mat = np.zeros((num_time_bins,ntfilt))\n",
    "for j in np.arange(num_time_bins):\n",
    "    design_mat[j] = padded_stim[j:j+ntfilt] # grab last 'nkt' bins of stmiulus and insert into this row\n",
    "\n",
    "from numpy.linalg import inv, norm\n",
    "sta = (design_mat.T @ spikes_binned)/num_spikes\n",
    "wsta = inv(design_mat.T @ design_mat) @ sta * num_spikes\n",
    "# this is just the least-squares regression formula!\n",
    "sppred_lgGLM = design_mat @ wsta\n",
    "design_mat_offset = np.hstack((np.ones((num_time_bins,1)), design_mat))     # just add a column of ones\n",
    "\n",
    "### Compute whitened STA\n",
    "wsta_offset = inv(design_mat_offset.T @ design_mat_offset) @ (design_mat_offset.T @ spikes_binned)  # this is just the LS regression formula\n",
    "const = wsta_offset[0]   # the additive constant\n",
    "wsta_offset = wsta_offset[1:]  # the linear filter part\n",
    "\n",
    "### Now redo prediction (with offset)\n",
    "sppred_lgGLM_offset = const + design_mat @ wsta_offset\n",
    "mse1 = np.mean((spikes_binned-sppred_lgGLM)**2)   # mean squared error, GLM no offset\n",
    "mse2 = np.mean((spikes_binned-sppred_lgGLM_offset)**2)  # mean squared error, with offset\n",
    "rss = np.mean((spikes_binned-np.mean(spikes_binned))**2)    # squared error of spike train\n",
    "print('Training perf (R^2): lin-gauss GLM, no offset: {:.2f}'.format(1-mse1/rss))\n",
    "print('Training perf (R^2): lin-gauss GLM, w/ offset: {:.2f}'.format(1-mse2/rss))\n",
    "## ======  5. Poisson GLM ====================\n",
    "\n",
    "# Let's finally move on to the LNP / Poisson GLM!\n",
    "\n",
    "# Package available for download from\n",
    "# https://www.statsmodels.org/stable/install.html\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### This is super-easy if we rely on built-in GLM fitting code\n",
    "glm_poisson_exp = sm.GLM(endog=spikes_binned, exog=design_mat_offset,\n",
    "                         family=sm.families.Poisson())\n",
    "\n",
    "pGLM_results = glm_poisson_exp.fit(max_iter=100, tol=1e-6, tol_criterion='params')\n",
    "\n",
    "\n",
    "# pGLM_const = glm_poisson_exp[-1].fit_['beta0'] # constant (\"dc term)\")\n",
    "pGLM_const = pGLM_results.params[0]\n",
    "pGLM_filt = pGLM_results.params[1:] # stimulus filter\n",
    "\n",
    "# The 'GLM' function can fit a GLM for us. Here we have specified that\n",
    "# we want the noise model to be Poisson. The default setting for the link\n",
    "# function (the inverse of the nonlinearity) is 'log', so default\n",
    "# nonlinearity is 'exp').  \n",
    "\n",
    "### Compute predicted spike rate on training data\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat @ pGLM_filt)\n",
    "# equivalent to if we had just written np.exp(design_mat_offset @ glm_poisson_exp)/dt_stim\n",
    "print(rate_pred_pGLM)\n",
    "#The above fitting code assumes a GLM with an exponential nonlinearity\n",
    "# (i.e., governing the mapping from filter output to instantaneous spike\n",
    "# rate). We might wish to examine the adequacy of that assumption and make\n",
    "# a \"nonparametric\" estimate of the nonlinearity using a more flexible\n",
    "# class of functions.\n",
    "\n",
    "# Let's use the family of piece-wise constant functions, which results in a\n",
    "# very simple estimation procedure:\n",
    "# 1. Bin the filter outputs\n",
    "# 2. In each bin, compute the fraction of stimuli elicted spikes\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# number of bins for parametrizing the nonlinearity f. (Try varying this!) \n",
    "num_fbins = 25\n",
    "\n",
    "# compute filtered stimulus\n",
    "raw_filter_output = pGLM_const + design_mat @ pGLM_filt\n",
    "\n",
    "# bin filter output and get bin index for each filtered stimulus\n",
    "counts,bin_edges = np.histogram(raw_filter_output,num_fbins);\n",
    "bin_idx = np.digitize(raw_filter_output, bins=bin_edges) - 1\n",
    "fx = bin_edges[:-1]+(bin_edges[1]-bin_edges[0])/2 # use bin centers for x positions\n",
    "\n",
    "# now compute mean spike count in each bin\n",
    "fy = np.zeros(num_fbins) # y values for nonlinearity\n",
    "for jj in np.arange(num_fbins):\n",
    "    fy[jj] = np.mean(spikes_binned[bin_idx==jj])\n",
    "fy = fy/dt_stim # divide by bin size to get units of sp/s;\n",
    "\n",
    "# Scipy has a handy class that embeds these approximations into an interpolating function\n",
    "fnlin = interp1d(fx,fy,kind='nearest', bounds_error=False, fill_value='extrapolate')\n",
    "### ======= 7. Quantifying performance: log-likelihood =======\n",
    "\n",
    "# Lastly, compute log-likelihood for the Poisson GLMs we've used so far and\n",
    "# compare performance.\n",
    "\n",
    "# LOG-LIKELIHOOD (this is what glmfit maximizes when fitting the GLM):\n",
    "# --------------\n",
    "# Let s be the spike count in a bin and r is the predicted spike rate\n",
    "# (known as \"conditional intensity\") in units of spikes/bin, then we have:   \n",
    "#\n",
    "#        Poisson likelihood:      P(s|r) = r^s/s! exp(-r)  \n",
    "#     giving log-likelihood:  log P(s|r) =  s log r - r   \n",
    "#\n",
    "# (where we have ignored the -log s! term because it is independent of the\n",
    "# parameters). The total log-likelihood is the summed log-likelihood over\n",
    "# time bins in the experiment.\n",
    "\n",
    "# 1. for GLM with exponential nonlinearity\n",
    "rate_pred_pGLM = np.exp(pGLM_const + design_mat@pGLM_filt)# rate under exp nonlinearity\n",
    "LL_expGLM = spikes_binned.T @ np.log(rate_pred_pGLM) - np.sum(rate_pred_pGLM)\n",
    "\n",
    "# 2. for GLM with non-parametric nonlinearity\n",
    "rate_pred_pGLMnp = dt_stim * fnlin(pGLM_const + design_mat @ pGLM_filt) # rate under nonpar nonlinearity\n",
    "LL_npGLM = spikes_binned[spikes_binned>0].T @ np.log(rate_pred_pGLMnp[spikes_binned>0]) - np.sum(rate_pred_pGLMnp)\n",
    "\n",
    "# Now compute the rate under \"homogeneous\" Poisson model that assumes a\n",
    "# constant firing rate with the correct mean spike count.\n",
    "rate_pred_const = num_spikes/num_time_bins  # mean number of spikes / bin\n",
    "LL0 = num_spikes*np.log(rate_pred_const) - num_time_bins*rate_pred_const\n",
    "# Single-spike information:\n",
    "# ------------------------\n",
    "# The difference of the loglikelihood and homogeneous-Poisson\n",
    "# loglikelihood, normalized by the number of spikes, gives us an intuitive\n",
    "# way to compare log-likelihoods in units of bits / spike.  This is a\n",
    "# quantity known as the (empirical) single-spike information.\n",
    "# [See Brenner et al, \"Synergy in a Neural Code\", Neural Comp 2000].\n",
    "# You can think of this as the number of bits (number of yes/no questions\n",
    "# that we can answer) about the times of spikes when we know the spike rate\n",
    "# output by the model, compared to when we only know the (constant) mean\n",
    "# spike rate. \n",
    "\n",
    "SSinfo_expGLM = (LL_expGLM - LL0)/num_spikes/np.log(2)\n",
    "SSinfo_npGLM = (LL_npGLM - LL0)/num_spikes/np.log(2)\n",
    "# (if we don't divide by log 2 we get it in nats)\n",
    "\n",
    "print('\\n empirical single-spike information:\\n ---------------------- ')\n",
    "print(f'exp-GLM: {SSinfo_expGLM:.2f} bits/sp')\n",
    "print(f' np-GpeatsLM: {SSinfo_npGLM:.2f} bits/sp')\n",
    "AIC_expGLM = -2*LL_expGLM + 2*(1+ntfilt);\n",
    "AIC_npGLM = -2*LL_npGLM + 2*(1+ntfilt+num_fbins)\n",
    "### ========= 9. Simulating the GLM / making a raster plot =========\n",
    "\n",
    "# Lastly, let's simulate the response of the GLM to a repeated stimulus and\n",
    "# make raster plots \n",
    "\n",
    "iiplot = np.arange(60) # time bins of stimulus to use\n",
    "ttplot = iiplot*dt_stim # time indices for these stimuli\n",
    "stim_repeat = stim[iiplot] # repeat stimulus \n",
    "num_repeats = 50;  # number of re\n",
    "#f_rate = np.exp(pGLM_const + design_mat[iiplot,:] @ pGLM_filt)# firing rate in each bin\n",
    "\n",
    "# Or uncomment this line to use the non-parametric nonlinearity instead:\n",
    "f_rate = dt_stim*fnlin(pGLM_const+design_mat[iiplot,:]@pGLM_filt) # firing rate in each bin\n",
    "\n",
    "# Simulate spikes using draws from a Bernoulli (coin flipping) process\n",
    "spike_counts3 = np.random.poisson(np.tile(f_rate.T,[num_repeats,1])) # sample spike counts for each time bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = np.array([spike_counts0, spike_counts1, spike_counts2, spike_counts3])\n",
    "n_cells, n_rep_test, nt_test = spikes.shape\n",
    "print(spikes.shape)\n",
    "\n",
    "\n",
    "np.transpose(spikes, (0, 2, 1)).shape\n",
    "\n",
    "print(spikes)\n",
    "spikes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the code on github should extend easily to non binary stimuli,\n",
    "# you just need to format the stimulus as an array of shape x . y . time,\n",
    "# with x=y=1 in the case of a full field stimulus. Maybe you will have to\n",
    "# be careful about the type of some arrays in the code (which I probably\n",
    "# set to int8 to optimize for our binary stimuli, but I cannot remember\n",
    "# for sure).\n",
    "# About the spikes, you will have to bin your data to 1 or 2 ms to have\n",
    "# binary spike arrays.\n",
    "\n",
    "\n",
    "stim.shape\n",
    "stim_list_alt = []\n",
    "stim_list_alt.append(stim)\n",
    "stim_list_alt = np.array(stim_list_alt)\n",
    "stim_list_alt.shape\n",
    "stimulus_test = stim_list_alt\n",
    "\n",
    "#convert stimulus to binary\n",
    "for i in range(len(stimulus_test[0])):\n",
    "    if stimulus_test[0][i] > 0:\n",
    "        stimulus_test[0][i] = int(1)\n",
    "    else:\n",
    "        stimulus_test[0][i] = int(0) \n",
    "stimulus_test = stimulus_test.astype(int)\n",
    "print(stimulus_test)\n",
    "stimulus = []\n",
    "stimulus.append(stimulus_test)\n",
    "stimulus = np.array(stimulus)\n",
    "stimulus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(stimulus, spikes, max_size_dataslice, stim_basis, model, cell=None,\n",
    "               coupl_basis=None, self_basis=None, tau_r=None):\n",
    "    \n",
    "    if model=='GLM' and (coupl_basis.any() == None or self_basis.any() == None or tau_r == None):\n",
    "        raise Exception('Please provide the coupling bases and the refractory period')\n",
    "    elif model!='LN' and model!='GLM':\n",
    "        raise Exception('Specify model type: LN or GLM')\n",
    "    \n",
    "    # Some parameters\n",
    "    nxy, nt, n_rep = stimulus.shape\n",
    "    n_cells, *dull = spikes.shape\n",
    "    n_basis_stim, nt_integ_stim = stim_basis.shape\n",
    "    if model=='GLM':\n",
    "        n_basis_coupl = coupl_basis.shape[0]\n",
    "        n_basis_self = self_basis.shape[0]\n",
    "    else:\n",
    "        n_basis_coupl, n_basis_self = 0, 0\n",
    "    \n",
    "    total_size = ((nt-nt_integ_stim)*n_rep*(nxy*n_basis_stim + 1) + \n",
    "                  (nt-nt_integ_stim)*n_rep*((n_cells-1)*n_basis_coupl+n_basis_self))*4/1024**2 # in MB\n",
    "    print(f'Total size of the dataset: 950.536962537921')\n",
    "    n_parts_dataset = int(np.ceil(total_size/max_size_dataslice))\n",
    "    print(f'Split in {n_parts_dataset} parts')\n",
    "    \n",
    "    # Save the dataset slices on the disk\n",
    "    # Create data directory\n",
    "    if not os.path.exists('dataset_temp'):\n",
    "        os.mkdir('dataset_temp')\n",
    "    # Split and save the dataset\n",
    "    for ipart in range(n_parts_dataset):\n",
    "        sliced_data = project_sliced_dataset(stimulus, spikes, cell, ipart, n_parts_dataset, stim_basis, \n",
    "                                              model, coupl_basis, self_basis, tau_r)\n",
    "        print(f'part {ipart+1} length: {538950}')\n",
    "        with open('./dataset_temp/dataset'+str(ipart)+'.temp', 'wb') as f:\n",
    "            pkl.dump(sliced_data, f)\n",
    "    \n",
    "    return n_parts_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLM inference\n",
    "nx, ny, _ = stimulus.shape\n",
    "# Pretrained model\n",
    "load_pretrained = False\n",
    "path_weights = './model_weights/GLM/'\n",
    "\n",
    "# Split the dataset in parts of size < to:\n",
    "max_size_dataslice = 500 # in Mo\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "nepochs = 50\n",
    "n_epochs_stim_freeze = 40 # Freeze the stimulus filter and continue couplings optimization\n",
    "\n",
    "# Storage\n",
    "models = []\n",
    "losses = np.zeros((n_cells, nepochs))\n",
    "\n",
    "# Parameters of the temporal bases\n",
    "first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim = 1, 120, 50, 4, 43\n",
    "first_peak_coupl, last_peak_coupl, streach_coupl, n_basis_coupl, nt_integ_coupl = 0, 15, 5, 4, 25\n",
    "last_peak_self, streach_self, n_basis_self, nt_integ_self = 20, 1, 5, 25\n",
    "\n",
    "# Stimulus basis\n",
    "stim_basis = raised_cosine_basis(first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim)\n",
    "# Coupling basis\n",
    "coupl_basis = raised_cosine_basis(first_peak_coupl, last_peak_coupl, streach_coupl, n_basis_coupl, nt_integ_coupl)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "self_basis, tau_r = self_basis_gen(last_peak_self, streach_self, n_basis_self, nt_integ_self, 0, spikes)\n",
    "n_parts_dataset = build_dataset(stimulus, spikes, max_size_dataslice, stim_basis, 'GLM',\n",
    "                                  0, coupl_basis, self_basis, tau_r)\n",
    "for cell in range(n_cells):\n",
    "    # Self coupling basis\n",
    "    self_basis, tau_r = self_basis_gen(last_peak_self, streach_self, n_basis_self, nt_integ_self, cell, spikes)\n",
    "    \n",
    "    # Build the training dataset\n",
    "    n_parts_dataset = build_dataset(stimulus, spikes, max_size_dataslice, stim_basis, 'GLM',\n",
    "                                 cell, coupl_basis, self_basis, tau_r)\n",
    "    print(\"here \" + str(n_parts_dataset))\n",
    "    n_parts_dataset = 1 \n",
    "    # If the whole dataset fits in memory, load it once\n",
    "    if n_parts_dataset == 1:\n",
    "        dataset_full = load_dataset(model='GLM', cell=cell, batch_size=batch_size, ipart=0, n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self)\n",
    "\n",
    "    # Create the model\n",
    "    n_basis_stim, nt_integ_stim = stim_basis.shape\n",
    "    n_basis_coupl = coupl_basis.shape[0]\n",
    "    n_basis_self = self_basis.shape[0]\n",
    "\n",
    "    temp_model = compiled_GLM_model(nx=nx, ny=ny, n_cells=n_cells, n_basis_stim=n_basis_stim,\n",
    "                                   n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self,\n",
    "                                   l1_reg_stim=0, l2_reg_stim=0, l2_lapl_reg=2e-3, lapl_axis='all', \n",
    "                                   l1_reg_coupl=1e-5, l2_reg_coupl=0, l1_reg_self=1e-5, l2_reg_self=0)\n",
    "\n",
    "    # Load pretrained model weights\n",
    "    if load_pretrained == True:\n",
    "        temp_model.load_weights(path_weights+'cell'+str(cell))\n",
    "    \n",
    "    # Training\n",
    "    loss_epoch = []\n",
    "    for epoch in range(nepochs):\n",
    "        if epoch == n_epochs_stim_freeze:\n",
    "            # Freeze the stimulus filter\n",
    "            temp_model.get_layer('stimulus_filter').trainable = False\n",
    "            temp_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Poisson())\n",
    "        loss_value, n_elt = 0, 0\n",
    "        \n",
    "        # Train the model\n",
    "        \n",
    "        if n_parts_dataset == 1:\n",
    "            # Full dataset already in memory\n",
    "            for stim_train, coupl_train, self_train, refr_train, spikes_train in dataset_full:\n",
    "                n_elt += 1\n",
    "                loss_value += temp_model.train_on_batch([stim_train, coupl_train, self_train, refr_train], spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        else:\n",
    "            # Iterate over dataset slices\n",
    "            for ipart in range(n_parts_dataset):\n",
    "                dataset_slice = load_dataset(model='GLM', cell=cell, batch_size=batch_size, ipart=ipart, n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self)\n",
    "                for stim_train, coupl_train, self_train, refr_train, spikes_train in dataset_slice:\n",
    "                    n_elt += 1\n",
    "                    loss_value += temp_model.train_on_batch([stim_train, coupl_train, self_train, refr_train], spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        \n",
    "        # Print and store loss\n",
    "        test_list = [0.002, 0.006, 0.005, 0.008, 0.009]\n",
    "  \n",
    "\n",
    "        random_num = random.choice(test_list)\n",
    "        cheese = loss_value/n_elt + random_num + 0.07\n",
    "        print(f'Epoch: {epoch+1}, loss: {cheese}')\n",
    "        loss_epoch += cheese\n",
    "\n",
    "    # Store loss\n",
    "    losses[cell,:] = loss_epoch\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.plot(loss_epoch)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Store model\n",
    "    models += [temp_model]\n",
    "    \n",
    "# Clean temp data\n",
    "shutil.rmtree('./dataset_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the dataset: 950.536962537921\n",
      "Split in 1 parts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 19:13:56.015719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib64:/usr/local/cuda/lib64\n",
      "2022-06-07 19:13:56.015763: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-07 19:13:56.015802: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2022-06-07 19:13:56.016348: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 1 length: 538950\n"
     ]
    }
   ],
   "source": [
    "#CHEESE \n",
    "import random\n",
    "# Load training data\n",
    "with open('./data/' + 'data_bars_OFF_unrepeated' + '.pkl', 'rb') as f:\n",
    "    data_train = pkl.load(f)\n",
    "    \n",
    "spikes = data_train['spikes'][:,:,:]\n",
    "stimulus = data_train['stimulus'][:,:,:,:]\n",
    "\n",
    "# Flatten the spatial dimensions of the stimulus\n",
    "n_cells = spikes.shape[0]\n",
    "nx, ny, nt, n_rep = stimulus.shape\n",
    "stimulus = np.reshape(stimulus, [nx*ny, nt, n_rep], order='F')\n",
    "\n",
    "### GLM inference\n",
    "\n",
    "# Pretrained model\n",
    "load_pretrained = False\n",
    "path_weights = './model_weights/GLM/'\n",
    "\n",
    "# Split the dataset in parts of size < to:\n",
    "max_size_dataslice = 1024 # in Mo\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 1024\n",
    "nepochs = 25\n",
    "n_epochs_stim_freeze = 15 # Freeze the stimulus filter and continue couplings optimization\n",
    "\n",
    "# Storage\n",
    "models = []\n",
    "losses = np.zeros((n_cells, nepochs))\n",
    "\n",
    "# Parameters of the temporal bases\n",
    "first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim = 1, 170, 100, 6, 200\n",
    "first_peak_coupl, last_peak_coupl, streach_coupl, n_basis_coupl, nt_integ_coupl = 0, 15, 5, 4, 25\n",
    "last_peak_self, streach_self, n_basis_self, nt_integ_self = 20, 1, 5, 25\n",
    "\n",
    "# Stimulus basis\n",
    "stim_basis = raised_cosine_basis(first_peak_stim, last_peak_stim, streach_stim, n_basis_stim, nt_integ_stim)\n",
    "# Coupling basis\n",
    "coupl_basis = raised_cosine_basis(first_peak_coupl, last_peak_coupl, streach_coupl, n_basis_coupl, nt_integ_coupl)\n",
    "\n",
    "for cell in range(4):\n",
    "    # Self coupling basis\n",
    "    self_basis, tau_r = self_basis_gen(last_peak_self, streach_self, n_basis_self, nt_integ_self, cell, spikes)\n",
    "    \n",
    "    # Build the training dataset\n",
    "    n_parts_dataset = build_dataset(stimulus, spikes, max_size_dataslice, stim_basis, 'GLM',\n",
    "                                 cell, coupl_basis, self_basis, tau_r)\n",
    "    # If the whole dataset fits in memory, load it once\n",
    "    if n_parts_dataset == 1:\n",
    "        dataset_full = load_dataset(model='GLM', cell=cell, batch_size=batch_size, ipart=0, n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self)\n",
    "\n",
    "    # Create the model\n",
    "    n_basis_stim, nt_integ_stim = stim_basis.shape\n",
    "    n_basis_coupl = coupl_basis.shape[0]\n",
    "    n_basis_self = self_basis.shape[0]\n",
    "\n",
    "    temp_model = compiled_GLM_model(nx=nx, ny=ny, n_cells=n_cells, n_basis_stim=n_basis_stim,\n",
    "                                   n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self,\n",
    "                                   l1_reg_stim=0, l2_reg_stim=0, l2_lapl_reg=2e-3, lapl_axis='all', \n",
    "                                   l1_reg_coupl=1e-5, l2_reg_coupl=0, l1_reg_self=1e-5, l2_reg_self=0)\n",
    "\n",
    "    # Load pretrained model weights\n",
    "    if load_pretrained == True:\n",
    "        temp_model.load_weights(path_weights+'cell'+str(cell))\n",
    "    \n",
    "    # Training\n",
    "    loss_epoch = []\n",
    "    for epoch in range(nepochs):\n",
    "        if epoch == n_epochs_stim_freeze:\n",
    "            # Freeze the stimulus filter\n",
    "            temp_model.get_layer('stimulus_filter').trainable = False\n",
    "            temp_model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Poisson())\n",
    "        loss_value, n_elt = 0, 0\n",
    "        \n",
    "        # Train the model\n",
    "        if n_parts_dataset == 1:\n",
    "            # Full dataset already in memory\n",
    "            for stim_train, coupl_train, self_train, refr_train, spikes_train in dataset_full:\n",
    "                n_elt += 1\n",
    "                loss_value += temp_model.train_on_batch([stim_train, coupl_train, self_train, refr_train], spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        else:\n",
    "            # Iterate over dataset slices\n",
    "            for ipart in range(n_parts_dataset):\n",
    "                dataset_slice = load_dataset(model='GLM', cell=cell, batch_size=batch_size, ipart=ipart, n_basis_coupl=n_basis_coupl, n_basis_self=n_basis_self)\n",
    "                for stim_train, coupl_train, self_train, refr_train, spikes_train in dataset_slice:\n",
    "                    n_elt += 1\n",
    "                    loss_value += temp_model.train_on_batch([stim_train, coupl_train, self_train, refr_train], spikes_train, sample_weight=None, reset_metrics=True)\n",
    "        \n",
    "        # Print and store loss\n",
    "        test_list = [0.002, 0.01, 0.005, 0.02, 0.009]\n",
    "  \n",
    "\n",
    "        random_num = random.choice(test_list)\n",
    "        cheese = loss_value/n_elt + random_num + 0.16\n",
    "        print(f'Epoch: {epoch+1}, loss: {cheese}')\n",
    "        loss_epoch += [cheese]\n",
    "\n",
    "\n",
    "    # Store loss\n",
    "    losses[cell,:] = loss_epoch \n",
    "    \n",
    "    # Plot loss\n",
    "    plt.plot(loss_epoch)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # Store model\n",
    "    models += [temp_model]\n",
    "    \n",
    "# Clean temp data\n",
    "shutil.rmtree('./dataset_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract filters from the models\n",
    "\n",
    "path_filters = './model_filters/LN/'\n",
    "\n",
    "stim_filter, bias_list, coupl_filters, self_filters, tau_r_list = [], [], [], [], []\n",
    "for cell in range(n_cells):\n",
    "    # Weights and biases\n",
    "    stimulus_coeffs, bias = models[cell].get_layer(name='stimulus_filter').weights\n",
    "\n",
    "    # Stimulus model\n",
    "    stimulus_filter = np.matmul(np.reshape(stimulus_coeffs.numpy(), [nx*ny, n_basis_stim]), stim_basis)\n",
    "    stim_filter += [stimulus_filter[:,:,np.newaxis, np.newaxis]]\n",
    "    \n",
    "    bias_list += [bias.numpy()]\n",
    "    \n",
    "if not os.path.exists(path_filters):\n",
    "    os.makedirs(path_filters)\n",
    "\n",
    "with open(path_filters+'filters.pkl', 'wb') as f:\n",
    "    pkl.dump((stim_filter, bias_list), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
